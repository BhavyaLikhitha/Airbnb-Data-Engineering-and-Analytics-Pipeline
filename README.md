<div align="center">

<img src="docs/airbnb_logo.png" alt="Airbnb Logo" width="120" />

# Airbnb Data Engineering & Analytics Pipeline

**End-to-end cloud data pipeline ‚Äî from raw CSVs to executive dashboards**

![AWS S3](https://img.shields.io/badge/AWS_S3-232F3E?style=flat-square&logo=amazons3&logoColor=white)
![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=flat-square&logo=snowflake&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-FF694B?style=flat-square&logo=dbt&logoColor=white)
![Looker Studio](https://img.shields.io/badge/Looker_Studio-4285F4?style=flat-square&logo=looker&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-336791?style=flat-square&logo=postgresql&logoColor=white)
![Jinja](https://img.shields.io/badge/Jinja-B41717?style=flat-square&logo=jinja&logoColor=white)

*Ingests Airbnb CSV data from AWS S3 ‚Üí loads into Snowflake ‚Üí transforms via dbt (Bronze ‚Üí Silver ‚Üí Gold) ‚Üí serves insights through Looker Studio*

</div>

---

## üìã Table of Contents

- [Project Overview](#-project-overview)
- [Dashboard](#-dashboard)
- [Architecture & Tech Stack](#-architecture--tech-stack)
- [What Makes This Project Unique](#-what-makes-this-project-unique)
- [Key Design Decisions](#-key-design-decisions)
- [Dataset Details](#-dataset-details)
- [Data Flow](#-data-flow)
- [Project Setup Guide](#-project-setup-guide)
- [dbt Transformations ‚Äî Full Coverage](#-dbt-transformations--full-coverage)
- [Looker Studio ‚Äî Dashboard Details](#-looker-studio--dashboard-details)
- [How to Run the Pipeline](#-how-to-run-the-pipeline)
- [Future Improvements & Scope](#-future-improvements--scope)

---

## üîç Project Overview

### Objective

Build a production-grade analytics engineering pipeline that takes raw, messy Airbnb data and transforms it into clean, trustworthy, analytics-ready datasets ‚Äî served through an executive dashboard.

### Business Context

Raw Airbnb data is not directly analysis-ready. It suffers from quality issues, mixed granularity, missing values, and operational complexity. Business teams need reliable KPIs like **total revenue**, **average booking value**, **superhost performance**, and **market-level breakdowns** ‚Äî but they can't get them from raw CSVs.

This project solves that by creating a fully reproducible pipeline that standardizes ingestion from cloud storage, applies business transformations with built-in data quality checks, supports historical tracking through SCD Type 2 snapshots, and exposes consistent, analytics-ready datasets for BI reporting.

---

## üìä Dashboard (Looker Studio)

<div align="center">

<img src="docs/dashboard.png" alt="Looker Studio Dashboard" width="850" />

<br /><br />

[![Explore Live Dashboard](https://img.shields.io/badge/üîó_Explore_Live_Dashboard-4285F4?style=for-the-badge&logoColor=white)](https://lookerstudio.google.com/reporting/713b7567-f53d-47a1-8ecd-9fc900541e72)

</div>

---

## üèó Architecture & Tech Stack

<div align="center">

<img src="docs/architecture.png" alt="Architecture Diagram" width="850" />

</div>

<br />

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Storage** | AWS S3 | Cloud storage for raw CSV source files |
| **Access Control** | AWS IAM | Least-privilege access policies for S3 |
| **Data Warehouse** | Snowflake | Scalable cloud warehouse for all data layers |
| **Transformation** | dbt Core + dbt-snowflake | Layered SQL transformations with testing |
| **Language** | SQL + Jinja | Models, macros, and dynamic SQL generation |
| **Environment** | `uv` | Fast Python dependency & environment management |
| **Dashboarding** | Looker Studio | Executive-facing interactive dashboard |

---

## ‚ú® What Makes This Project Unique

This isn't just an ETL pipeline ‚Äî it's a **complete analytics engineering system** designed with production patterns in mind.

### üîÑ SCD Type 2 Historical Tracking

Dimension tables (`DIM_BOOKINGS`, `DIM_LISTINGS`, `DIM_HOSTS`) use **dbt snapshots** with `dbt_valid_from` / `dbt_valid_to` columns to capture every historical change. If a host gains superhost status or a listing changes price, the history is preserved ‚Äî not overwritten. This enables true **point-in-time analytics** that most simple ETL pipelines lose.

**Why this matters:** Without SCD2, a host's superhost status change would silently overwrite the previous record. With it, you can answer questions like *"What was the superhost share in Q2 vs. Q3?"* or *"How did a listing's price change affect its booking volume over time?"* ‚Äî queries that require historical dimension state.

### ü•âü•àü•á Bronze ‚Üí Silver ‚Üí Gold Layered Architecture

A production-grade medallion architecture ensures clear separation of concerns:

- **Bronze** ‚Äî Raw ingestion and type casting. No business logic. Acts as a controlled entry point so upstream schema changes are isolated here.
- **Silver** ‚Äî Cleaning, deduplication, derived fields, null handling. This is where data becomes trustworthy.
- **Gold** ‚Äî Analytics-ready datasets optimized for consumption. Business users and dashboards only ever touch this layer.

**Why this matters:** Each layer is independently testable, debuggable, and maintainable. If a data quality issue appears in Gold, you trace it back through Silver to Bronze ‚Äî not through a tangled monolith of CTEs.

### üìê Dual Consumption Patterns ‚Äî OBT + Star Schema

The Gold layer supports two consumption models side by side:

- **OBT (One Big Table)** ‚Äî A wide, denormalized table where every dimension is pre-joined. Optimized for fast Looker Studio rendering and ad-hoc exploration. No joins needed at query time.
- **Star Schema** (`FACT` + `DIM_*`) ‚Äî Normalized structure with a central fact table and separate dimension tables. Provides cleaner semantics, easier governance, and scalability for enterprise use cases and role-based access.

**Why both?** OBT is materialized as a **table** (not a view) because Looker Studio performs significantly better when scanning a single pre-computed table versus executing multi-table joins at query time. The star schema exists alongside it for analysts who need flexibility or for downstream systems that benefit from normalized data.

### üß™ Built-In Data Quality Testing

dbt tests validate data at every layer ‚Äî uniqueness on business keys, not-null constraints, referential integrity between fact and dimension tables, and accepted value checks on categorical fields like `BOOKING_STATUS`. Tests run as part of `dbt build`, meaning **no model reaches Gold without passing quality gates**.

### üß© Reusable Jinja Macros

Custom macros reduce repetitive SQL and enforce consistency across models. Rather than copy-pasting the same CASE logic or column transformations across 9+ model files, macros centralize that logic so a single change propagates everywhere. See the [Custom Macros](#custom-macros) section for details and code examples.

---

## üß† Key Design Decisions

Every tool and pattern choice in this project was made intentionally. This section explains the *why* behind the key decisions.

### Why Ephemeral Models for Gold Intermediates?

The `gold/ephemeral/` folder contains intermediate models (`bookings.sql`, `hosts.sql`, `listings.sql`) materialized as **ephemeral** ‚Äî meaning they compile as CTEs inlined into the downstream Gold models, not as physical tables or views in Snowflake.

**Reasoning:** These intermediates exist purely to prepare dimension-level logic (joins, filters, derivations) before they feed into `obt.sql` and `fact.sql`. Materializing them as tables would create unnecessary storage and maintenance overhead for objects that no one queries directly. Ephemeral keeps the DAG clean while giving us modular, testable SQL files.

```
ephemeral/bookings.sql  ‚îÄ‚îÄ‚îê
ephemeral/hosts.sql     ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂  obt.sql (materialized as TABLE)
ephemeral/listings.sql  ‚îÄ‚îÄ‚îò     fact.sql (materialized as TABLE)
```

### Why External Stage over Storage Integration?

Snowflake supports two patterns for loading from S3: **external stages** (with credentials passed at copy time) and **storage integrations** (IAM role-based trust between Snowflake and AWS).

This project uses an **external stage** for simplicity and portability ‚Äî it works out of the box without requiring Snowflake ACCOUNTADMIN-level setup or cross-account IAM trust configuration. For a production deployment, upgrading to a **storage integration with IAM roles** is recommended (no credentials in SQL, automatic credential rotation, tighter security boundary).

### Why `generate_schema_name` Macro?

By default, dbt writes all models to a single schema (whatever's in `profiles.yml`). This project overrides `generate_schema_name` to **route models to separate Snowflake schemas based on their layer** ‚Äî Bronze models land in `AIRBNB.BRONZE`, Silver in `AIRBNB.SILVER`, Gold in `AIRBNB.GOLD` ‚Äî not all piled into `AIRBNB.DEV`.

This mirrors production data warehouse conventions where each layer has its own schema with separate access controls, and makes it trivial to grant BI tools read-only access to Gold while restricting Bronze/Silver. See [`macros/generate_schema_name.sql`](airbnb_dbt_pipeline/macros/generate_schema_name.sql) for the implementation.

### Why OBT Is Materialized as a Table (Not a View)?

The OBT joins three dimensions with the fact table ‚Äî a query pattern that Looker Studio would execute on every single dashboard load if OBT were a view. By materializing it as a **table**, Snowflake computes the joins once during `dbt run`, and all subsequent dashboard queries are simple scans against a pre-built result set.

**Impact:** Faster dashboard rendering, lower Snowflake compute costs (no repeated joins), and more predictable query performance for end users.

### Why IAM Least-Privilege with Scoped S3 Policies?

The IAM policy intentionally separates `s3:ListBucket` (on the bucket) from `s3:GetObject` (scoped to the `source/` prefix only). This ensures Snowflake can list and read source files but **cannot access other prefixes** in the bucket, write objects, or delete anything. This is a security best practice that limits blast radius if credentials are ever compromised.

---

## üìÅ Dataset Details

The pipeline ingests three core Airbnb datasets stored as CSVs in S3, totaling over **5,000+ rows** across all sources:

| Table | Description |
|-------|-------------|
| **Bookings** | Reservation records including dates, status, pricing, and duration |
| **Listings** | Property details ‚Äî type, room type, nightly rate, city, and country |
| **Hosts** | Host profiles with superhost status and join date |

---

## üîÄ Data Flow

```
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ       AWS S3 Bucket               ‚îÇ
                         ‚îÇ  s3://airbnb-etl-pipeline/source/ ‚îÇ
                         ‚îÇ                                    ‚îÇ
                         ‚îÇ  üìÑ bookings.csv                  ‚îÇ
                         ‚îÇ  üìÑ listings.csv                  ‚îÇ
                         ‚îÇ  üìÑ hosts.csv                     ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                          Snowflake STAGE + COPY INTO
                                        ‚îÇ
                                        ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ       Snowflake Raw Tables        ‚îÇ
                         ‚îÇ  BOOKINGS | LISTINGS | HOSTS      ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                   dbt run
                                        ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº                         ‚ñº                         ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ   BRONZE    ‚îÇ          ‚îÇ   SILVER    ‚îÇ          ‚îÇ      GOLD        ‚îÇ
     ‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ          ‚îÇ                  ‚îÇ
     ‚îÇ Standardize ‚îÇ   ‚îÄ‚îÄ‚ñ∂    ‚îÇ Clean &     ‚îÇ   ‚îÄ‚îÄ‚ñ∂    ‚îÇ ‚≠ê OBT           ‚îÇ
     ‚îÇ Type cast   ‚îÇ          ‚îÇ Enrich      ‚îÇ          ‚îÇ ‚≠ê FACT           ‚îÇ
     ‚îÇ Rename cols ‚îÇ          ‚îÇ Derive cols ‚îÇ          ‚îÇ ‚≠ê DIM_BOOKINGS   ‚îÇ
     ‚îÇ             ‚îÇ          ‚îÇ Dedup       ‚îÇ          ‚îÇ ‚≠ê DIM_LISTINGS   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ ‚≠ê DIM_HOSTS     ‚îÇ
                                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                 ‚îÇ
                                                                 ‚ñº
                                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                       ‚îÇ  Looker Studio   ‚îÇ
                                                       ‚îÇ  Dashboard       ‚îÇ
                                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚öô Project Setup Guide

This section walks through the complete infrastructure setup ‚Äî from S3 to Snowflake ‚Äî before any dbt models are run.

### Step 1 ‚Äî AWS S3: Create Bucket & Upload Data

Create an S3 bucket named `airbnb-etl-pipeline` with a `source/` prefix and upload the three CSV files (`bookings.csv`, `listings.csv`, `hosts.csv`).

### Step 2 ‚Äî AWS IAM: Configure Access

| Method | Description |
|--------|-------------|
| **IAM Role** *(recommended for production)* | Attach to a Snowflake storage integration ‚Äî no keys in SQL, automatic rotation |
| **IAM User** *(quick setup for development)* | Access key + secret key passed via `CREDENTIALS` clause |

Apply a least-privilege S3 policy ‚Äî `ListBucket` scoped to the bucket, `GetObject` scoped to only the `source/` prefix:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ListBucket",
      "Effect": "Allow",
      "Action": ["s3:ListBucket"],
      "Resource": "arn:aws:s3:::airbnb-etl-pipeline"
    },
    {
      "Sid": "ReadObjects",
      "Effect": "Allow",
      "Action": ["s3:GetObject"],
      "Resource": "arn:aws:s3:::airbnb-etl-pipeline/source/*"
    }
  ]
}
```

> **Why no `PutObject` or `DeleteObject`?** Snowflake only needs to *read* source files. Granting write access would violate least-privilege and increase risk if credentials are exposed.

### Step 3 ‚Äî Snowflake: Create Base Tables

Run `DDL/schema_creation.sql` to create `BOOKINGS`, `LISTINGS`, and `HOSTS` tables in Snowflake. These serve as the raw landing zone before dbt takes over.

### Step 4 ‚Äî Snowflake: Create File Format & External Stage

From `DDL/resources.sql`:

```sql
CREATE FILE FORMAT IF NOT EXISTS csv_format
  TYPE = 'CSV'
  FIELD_DELIMITER = ','
  SKIP_HEADER = 1
  ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE;

CREATE OR REPLACE STAGE snowstage
  FILE_FORMAT = csv_format
  URL = 's3://airbnb-etl-pipeline/source/';
```

> `ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE` is set intentionally to handle potential CSV variations gracefully during development. In production, you'd flip this to `TRUE` and add error handling.

### Step 5 ‚Äî Snowflake: Load Data from S3

```sql
COPY INTO BOOKINGS FROM @snowstage FILES = ('bookings.csv')
  CREDENTIALS = (aws_key_id='<AWS_KEY_ID>', aws_secret_key='<AWS_SECRET_KEY>');

COPY INTO LISTINGS FROM @snowstage FILES = ('listings.csv')
  CREDENTIALS = (aws_key_id='<AWS_KEY_ID>', aws_secret_key='<AWS_SECRET_KEY>');

COPY INTO HOSTS FROM @snowstage FILES = ('hosts.csv')
  CREDENTIALS = (aws_key_id='<AWS_KEY_ID>', aws_secret_key='<AWS_SECRET_KEY>');
```

> ‚ö†Ô∏è **Security:** Never hardcode AWS keys in SQL scripts. Use IAM roles or a secrets manager. Rotate credentials periodically and apply least-privilege Snowflake roles.

At this point, raw data is loaded in Snowflake and ready for dbt transformations.

---

## üîß dbt Transformations ‚Äî Full Coverage

### Project Structure

```
airbnb_dbt_pipeline/
‚îú‚îÄ‚îÄ dbt_project.yml              # Project config
‚îú‚îÄ‚îÄ profiles example.yml         # Snowflake connection template
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ sources/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sources.yml          # Source definitions (raw Snowflake tables)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_bookings.sql  # Raw standardization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_hosts.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bronze_listings.sql
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_bookings.sql  # Cleaning & enrichment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_hosts.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ silver_listings.sql
‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ       ‚îú‚îÄ‚îÄ obt.sql              # One Big Table (wide denormalized)
‚îÇ       ‚îú‚îÄ‚îÄ fact.sql             # Central fact table
‚îÇ       ‚îî‚îÄ‚îÄ ephemeral/
‚îÇ           ‚îú‚îÄ‚îÄ bookings.sql     # Ephemeral intermediate models
‚îÇ           ‚îú‚îÄ‚îÄ hosts.sql
‚îÇ           ‚îî‚îÄ‚îÄ listings.sql
‚îú‚îÄ‚îÄ snapshots/
‚îÇ   ‚îú‚îÄ‚îÄ dim_bookings.yml         # SCD2 snapshot config
‚îÇ   ‚îú‚îÄ‚îÄ dim_hosts.yml
‚îÇ   ‚îî‚îÄ‚îÄ dim_listings.yml
‚îú‚îÄ‚îÄ macros/
‚îÇ   ‚îú‚îÄ‚îÄ case_tags.sql            # Dynamic CASE statement generator
‚îÇ   ‚îú‚îÄ‚îÄ col_name_trim.sql        # Column name trimming utility
‚îÇ   ‚îú‚îÄ‚îÄ multiply.sql             # Multiplication helper
‚îÇ   ‚îî‚îÄ‚îÄ generate_schema_name.sql # Custom schema naming logic
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ source_tests.sql         # Custom data quality tests
‚îî‚îÄ‚îÄ analyses/
    ‚îú‚îÄ‚îÄ jinja_for_loop.sql       # Jinja loop examples
    ‚îî‚îÄ‚îÄ jinja_if_else.sql        # Jinja conditional examples
```

### Bronze Layer ‚Äî Raw Standardization

Maps raw Snowflake source tables to dbt models with consistent column naming and explicit data type casting. This layer acts as a controlled entry point ‚Äî no business logic, just standardization.

**What happens here:** Column names are normalized (e.g., trimming whitespace, consistent casing), data types are explicitly cast (strings to dates, numbers to appropriate numeric types), and source definitions in `sources.yml` create the contract between raw Snowflake tables and dbt.

**Why it matters:** If the upstream CSV schema ever changes (a column is renamed, a type shifts), the breakage is isolated to Bronze ‚Äî Silver and Gold remain untouched until Bronze is fixed.

### Silver Layer ‚Äî Cleaning & Enrichment

Handles the heavy lifting of data quality: null handling and default values, deduplication logic, derived fields (calculated metrics, date parts, booking revenue), and business-friendly column normalization.

**What happens here:** Nulls are replaced with meaningful defaults, duplicate records are identified and removed, calculated columns like `TOTAL_REVENUE` (price √ó nights) are derived, and date columns are broken into parts (year, month) for easier aggregation downstream.

### Gold Layer ‚Äî Analytics-Ready Output

Produces the final datasets consumed by Looker Studio:

| Model | Materialization | Description |
|-------|----------------|-------------|
| `obt.sql` | **Table** | One Big Table ‚Äî pre-joined, denormalized, optimized for fast BI scans |
| `fact.sql` | **Table** | Central fact table for star schema ‚Äî measures + foreign keys |
| `ephemeral/bookings.sql` | **Ephemeral** | Intermediate booking logic ‚Äî compiles as CTE, no physical table |
| `ephemeral/hosts.sql` | **Ephemeral** | Intermediate host logic ‚Äî compiles as CTE, no physical table |
| `ephemeral/listings.sql` | **Ephemeral** | Intermediate listing logic ‚Äî compiles as CTE, no physical table |

> **Why ephemeral?** These intermediates prepare dimension-level logic before feeding into OBT and FACT. Materializing them as tables would create unnecessary Snowflake objects that no one queries directly. Ephemeral keeps them modular in code but zero-cost in the warehouse.

### Snapshots ‚Äî SCD Type 2

Snapshot configs in `snapshots/` (`dim_bookings.yml`, `dim_hosts.yml`, `dim_listings.yml`) track historical changes for all three dimensions using dbt's YAML-based snapshot definitions with a timestamp strategy.

```bash
dbt snapshot
```

This creates versioned dimension tables (`DIM_BOOKINGS`, `DIM_LISTINGS`, `DIM_HOSTS`) with `dbt_valid_from` and `dbt_valid_to` columns. Each row represents a historical version ‚Äî the current record has `dbt_valid_to = NULL`.

**Example ‚Äî tracking a price change:**

| LISTING_ID | PRICE_PER_NIGHT | dbt_valid_from | dbt_valid_to |
|------------|----------------|----------------|--------------|
| L001 | 120 | 2024-01-01 | 2024-06-15 |
| L001 | 150 | 2024-06-15 | *NULL* |

The first row captures the historical price, the second is the current state. This makes time-travel queries straightforward ‚Äî join on `dbt_valid_from <= date AND (dbt_valid_to > date OR dbt_valid_to IS NULL)`.

### Custom Macros

Rather than repeating the same SQL patterns across 9+ model files, custom Jinja macros centralize reusable logic. All implementations are in [`macros/`](airbnb_dbt_pipeline/macros/).

| Macro | Purpose | Why It Exists |
|-------|---------|---------------|
| **`case_tags`** | Dynamically generates CASE expressions for categorizing records based on configurable thresholds (e.g., price segments: Premium / Mid-Range / Budget) | Eliminates repetitive CASE/WHEN blocks ‚Äî define categories once, reuse across any model |
| **`col_name_trim`** | Trims whitespace and applies consistent formatting to column names | Ensures Bronze layer standardization is uniform without manual column-by-column cleanup |
| **`multiply`** | Reusable multiplication helper for computed fields like `TOTAL_REVENUE = PRICE √ó NIGHTS` | Keeps calculated field logic DRY and consistent across bookings and fact models |
| **`generate_schema_name`** | Routes models to layer-specific Snowflake schemas (`BRONZE`, `SILVER`, `GOLD`) | Prevents all models from landing in a single schema ‚Äî enables schema-level access control |

### Data Quality Tests

```bash
dbt test
```

| Test Type | Applied To | Purpose |
|-----------|-----------|---------|
| `unique` | `BOOKING_ID`, `LISTING_ID`, `HOST_ID` | No duplicate business keys |
| `not_null` | `BOOKING_ID`, `LISTING_ID`, `HOST_ID` | No missing key values |
| `relationships` | Fact ‚Üí Dimensions | Referential integrity ‚Äî every FK in FACT exists in the corresponding DIM |
| `accepted_values` | `BOOKING_STATUS` | Only valid statuses (confirmed, cancelled, etc.) ‚Äî catches data corruption |
| **Custom** | `source_tests.sql` | Project-specific validation logic beyond built-in dbt tests |

Tests run as part of `dbt build`, which executes models, tests, and snapshots in dependency order ‚Äî meaning **no model reaches Gold without passing quality gates**.

---

## üìà Looker Studio ‚Äî Dashboard Details

The dashboard connects directly to the **Gold layer** in Snowflake via the Snowflake connector in Looker Studio. The primary data source is `AIRBNB.GOLD.OBT` ‚Äî a single pre-joined table that eliminates the need for Looker Studio to perform any joins at query time.

**KPIs displayed:**

| KPI | Description |
|-----|-------------|
| **Total Revenue** | Sum of all booking revenue |
| **Total Bookings** | Count of all bookings |
| **Avg Booking Value** | Average revenue per booking |
| **Avg Price per Night** | Average nightly rate across listings |
| **Superhost Share %** | Percentage of bookings from superhosts |

**Visualizations include:** Revenue over time (year-month trend), breakdown by property type, price segment analysis, top 5 countries & cities by revenue, and performance drill-down tables.

**Global Filters:** `BOOKING_DATE`, `COUNTRY`, `CITY`, `PROPERTY_TYPE`, `ROOM_TYPE`, `BOOKING_STATUS`

| Use Case | Recommended Model | Why |
|----------|-------------------|-----|
| Quick dashboard creation & ad-hoc analysis | `AIRBNB.GOLD.OBT` | Single table, no joins, fastest rendering |
| Advanced modeling & enterprise governance | `FACT` + `DIM_*` | Normalized, supports role-based access and complex queries |

---

## üöÄ How to Run the Pipeline

### 1. Clone the Repository

```bash
git clone https://github.com/BhavyaLikhitha/Airbnb-Data-Engineering-and-Analytics-Pipeline
cd airbnb-data-engineering-and-analytics-pipeline
```

### 2. Set Up Python Environment

```bash
uv init
uv sync
uv add dbt-core dbt-snowflake
```

### 3. Configure dbt Profile

Create `~/.dbt/profiles.yml` (or copy from `profiles example.yml`):

### 4. Validate Connection

```bash
dbt debug
```

### 5. Run the Full Pipeline

```bash
dbt run          # Build all models (Bronze ‚Üí Silver ‚Üí Gold)
dbt test         # Run all data quality tests
dbt snapshot     # Capture SCD Type 2 snapshots
dbt build        # Run + test + snapshot in dependency order
```

**Run a specific layer:**

```bash
dbt run --select bronze    # Only Bronze models
dbt run --select silver    # Only Silver models
dbt run --select gold      # Only Gold models
```

---

## üîÆ Future Improvements & Scope

| Improvement | Description |
|-------------|-------------|
| **Orchestration** | Add Airflow or GitHub Actions for automated, scheduled pipeline runs |
| **CI/CD** | Run `dbt test` on pull requests to catch data issues before merge |
| **Freshness Monitoring** | Add `dbt source freshness` checks with alerting for stale data |
| **Documentation Site** | Generate and host an interactive dbt docs site with `dbt docs generate` |
| **Row-Level Security** | Implement Snowflake row-level security for role-based BI access |
| **Incremental Models** | Convert heavy models to incremental for cost and performance gains |
| **Storage Integration** | Upgrade from external stage credentials to IAM role-based Snowflake storage integration |
| **Data Contracts** | Define explicit schemas at layer boundaries to catch breaking changes early |

---

<div align="center">

**Built with ‚ù§Ô∏è using dbt + Snowflake + AWS + Looker Studio**

</div>